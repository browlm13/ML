
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass[11pt]{article}

    
    
    \usepackage[T1]{fontenc}
    % Nicer default font (+ math font) than Computer Modern for most use cases
    \usepackage{mathpazo}

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % We will generate all images so they have a width \maxwidth. This means
    % that they will get their normal width if they fit onto the page, but
    % are scaled down if they would overflow the margins.
    \makeatletter
    \def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth
    \else\Gin@nat@width\fi}
    \makeatother
    \let\Oldincludegraphics\includegraphics
    % Set max figure width to be 80% of text width, for now hardcoded.
    \renewcommand{\includegraphics}[1]{\Oldincludegraphics[width=.8\maxwidth]{#1}}
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionLabelFormat{nolabel}{}
    \captionsetup{labelformat=nolabel}

    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    

    
    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatability definitions
    \def\gt{>}
    \def\lt{<}
    % Document parameters
    \title{HW1}
    
    
    

    % Pygments definitions
    
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % Exact colors from NB
    \definecolor{incolor}{rgb}{0.0, 0.0, 0.5}
    \definecolor{outcolor}{rgb}{0.545, 0.0, 0.0}



    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    \maketitle
    
    

    
    \hypertarget{csci-ua.0473-001-introduction-to-machine-learning}{%
\paragraph{CSCI-UA.0473-​001 Introduction to Machine
Learning}\label{csci-ua.0473-001-introduction-to-machine-learning}}

\hypertarget{homework-1}{%
\section{Homework 1}\label{homework-1}}

\hypertarget{name-laurence-lj-brown}{%
\subsubsection{Name: Laurence (LJ) Brown}\label{name-laurence-lj-brown}}

\hypertarget{due-oct.2-2019}{%
\subsubsection{Due: Oct.~2, 2019}\label{due-oct.2-2019}}

\hypertarget{goal-the-goal-of-this-homework-is-to-practice-implementing-a-logistic-regression-model-and-gradient-descent-as-well-as-to-explore-some-theoretical-concepts.}{%
\subsection{Goal: The goal of this homework is to practice implementing
a logistic regression model and gradient descent as well as to explore
some theoretical
concepts.}\label{goal-the-goal-of-this-homework-is-to-practice-implementing-a-logistic-regression-model-and-gradient-descent-as-well-as-to-explore-some-theoretical-concepts.}}

    You will need the following packages below to do the homework. Please DO
NOT import any other packages.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}1}]:} \PY{k+kn}{from} \PY{n+nn}{autograd} \PY{k}{import} \PY{n}{grad}
        \PY{k+kn}{import} \PY{n+nn}{autograd}\PY{n+nn}{.}\PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
        \PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
        \PY{k+kn}{from} \PY{n+nn}{sklearn}\PY{n+nn}{.}\PY{n+nn}{datasets}\PY{n+nn}{.}\PY{n+nn}{samples\PYZus{}generator} \PY{k}{import} \PY{n}{make\PYZus{}blobs}
\end{Verbatim}


    \hypertarget{problem-1-gradient-descent-40-pts-total}{%
\subsection{Problem 1: Gradient Descent (40 pts
total)}\label{problem-1-gradient-descent-40-pts-total}}

In this problem we will study gradient descent for optimization.

    \hypertarget{part-a-10pts-implementing-gradient-descent-with-a-fixed-learning-rate}{%
\subsubsection{Part (a) (10pts) Implementing gradient descent with a
fixed learning
rate}\label{part-a-10pts-implementing-gradient-descent-with-a-fixed-learning-rate}}

Using autograd, implement gradient descent with a fixed learning rate
for a general function.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}2}]:} \PY{k}{def} \PY{n+nf}{gradient\PYZus{}descent\PYZus{}fixed}\PY{p}{(}\PY{n}{fun}\PY{p}{,} \PY{n}{x0}\PY{p}{,} \PY{n}{lr}\PY{p}{,} \PY{n}{iterations}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{    Input:}
        \PY{l+s+sd}{        fun : function}
        \PY{l+s+sd}{        x0  : initial point}
        \PY{l+s+sd}{        lr  : fixed learning rate}
        \PY{l+s+sd}{        iterations : number of iterations to perform}
        
        \PY{l+s+sd}{    Return:}
        \PY{l+s+sd}{        x   : minimizer to fun}
        \PY{l+s+sd}{    \PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{n}{grad\PYZus{}fun} \PY{o}{=} \PY{n}{grad}\PY{p}{(}\PY{n}{fun}\PY{p}{)}
            
            \PY{n}{xi} \PY{o}{=} \PY{n}{x0}
            \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{iterations}\PY{p}{)}\PY{p}{:} 
                \PY{n}{xi} \PY{o}{=} \PY{n}{xi} \PY{o}{\PYZhy{}} \PY{n}{lr}\PY{o}{*}\PY{n}{grad\PYZus{}fun}\PY{p}{(}\PY{n}{xi}\PY{p}{)}
                
            \PY{k}{return} \PY{n}{xi}
\end{Verbatim}


    \hypertarget{part-b-10-pts-using-a-variable-learning-rate}{%
\subsubsection{Part (b) (10 pts) Using a variable learning
rate}\label{part-b-10-pts-using-a-variable-learning-rate}}

Sometimes it is necessary to decrease our learning rate as we iterate to
help gradient descent converge. Implement gradient descent below where
the learning rate at iteration \(i\) is given by \[
\mathrm{lr}_i = \frac{\mathrm{lr}_0}{i+1}.
\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}3}]:} \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{Input:}
        \PY{l+s+sd}{    fun : function}
        \PY{l+s+sd}{    x0  : initial point}
        \PY{l+s+sd}{    lr  : initial learning rate}
        \PY{l+s+sd}{    iterations : number of iterations to perform}
        \PY{l+s+sd}{    }
        \PY{l+s+sd}{Return:}
        \PY{l+s+sd}{    x   : minimizer to fun}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{k}{def} \PY{n+nf}{gradient\PYZus{}descent\PYZus{}variable}\PY{p}{(}\PY{n}{fun}\PY{p}{,} \PY{n}{x0}\PY{p}{,} \PY{n}{lr}\PY{p}{,} \PY{n}{iterations}\PY{p}{,} \PY{n}{iteration\PYZus{}offset}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{)}\PY{p}{:} 
            \PY{n}{grad\PYZus{}fun} \PY{o}{=} \PY{n}{grad}\PY{p}{(}\PY{n}{fun}\PY{p}{)}
            \PY{n}{alpha} \PY{o}{=} \PY{k}{lambda} \PY{n}{i}\PY{p}{:} \PY{n}{lr} \PY{o}{/} \PY{p}{(}\PY{n}{i}\PY{o}{+}\PY{l+m+mf}{1.0}\PY{p}{)}
            
            \PY{n}{xi} \PY{o}{=} \PY{n}{x0}
            \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{iterations}\PY{p}{)}\PY{p}{:} 
                \PY{n}{xi} \PY{o}{=} \PY{n}{xi} \PY{o}{\PYZhy{}} \PY{n}{alpha}\PY{p}{(}\PY{n}{i}\PY{o}{+}\PY{n}{iteration\PYZus{}offset}\PY{p}{)}\PY{o}{*}\PY{n}{grad\PYZus{}fun}\PY{p}{(}\PY{n}{xi}\PY{p}{)}
        
            \PY{k}{return} \PY{n}{xi}
\end{Verbatim}


    \hypertarget{part-c-10-pts-choosing-the-learning-rate}{%
\subsubsection{Part (c) (10 pts) Choosing the learning
rate}\label{part-c-10-pts-choosing-the-learning-rate}}

Let \(\alpha\) denote the learning rate and consider the function
\(f(x) = \frac{1}{2}x^2\). The gradient descent update rule at iteration
\(n+1\) is given by \[
x_{n+1} = x_n - \alpha f'(x_n)
\] Is there a critical value \(\alpha_0\) so that if
\(\alpha \ge \alpha_0\) then gradient descent will not converge for
\(f(x)\)? If so what is it and explain why?

    \hypertarget{convergent-alphas-for-fx-fracx22-gradient-descent}{%
\paragraph{\texorpdfstring{Convergent \(\alpha\)'s for
\(f(x) = \frac{x^2}{2}\) gradient
descent}{Convergent \textbackslash{}alpha's for f(x) = \textbackslash{}frac\{x\^{}2\}\{2\} gradient descent}}\label{convergent-alphas-for-fx-fracx22-gradient-descent}}

    The sequence,

\[
\lim_{n \rightarrow \infty} x_n,
\]

converges or diverges to/from the root of \(f\) depending on the
learning rate, \(\alpha \in \mathbb{R}\), and the initial guess,
\(x_0 \in \mathbb{R}\).

\begin{equation}
   \alpha \in (0,2) \text{ converges } \forall \, x_0 \text{, }
\end{equation} \begin{equation}
   \alpha \notin (0,2) \text{ diverges } \forall \, x_0 \neq 0
\end{equation}

    We can write the \(n^{th}\) term of the fixed-point iteration dirrectly
as,

\begin{equation} \tag{1}
x_n = \beta^n x_0
\end{equation}

\[
\\
\text{ where, } \\
\beta = 1 - \alpha
\]

    This comes from plugging in \(f'\) to the equation for an update step of
gradient descent,

\[
    x_{n} \, = \, x_{n-1} - \alpha f'(x_{n-1}) \, = \, (1 - \alpha)x_{n-1},
\]

\[
    f'(x) = x
\]

    Since we know the function, \(f(x) = \frac{x^{2}}{2}\), has a minimum at
\(x = 0\), then we can see that the iterates are only gaurenteed to
converge for \(|\beta| < 1\).

    \[
   \lim_{n \rightarrow \infty} |\beta^n x_0| = 
\begin{cases}
    0       &   \text{if } |\beta| < 1, \\
    0       &   \text{if } x_0 = 0, \\
    |x_0|   &   \text{if } |\beta| = 1, \\
    \infty  &   \text{if } |\beta| > 1, \\
\end{cases}
\]

    This leads to the convergent range of \(\alpha\)'s,

\[
|\beta| < 1
\]

\[
\Rightarrow |1 - \alpha| < 1
\]

\[
\Rightarrow -1 < 1 - \alpha < 1
\]

\[
\Rightarrow -2 < - \alpha < 0
\]

\[
\Rightarrow 2 > \alpha > 0
\]

\[
\alpha \in (0,2)
\]

    \hypertarget{part-d-10-pts-feature-scaling}{%
\subsubsection{Part (d) (10 pts) Feature
scaling}\label{part-d-10-pts-feature-scaling}}

Oftentimes it is advantageous to rescale or normalize our features. As a
toy example suppose we want to predict a person's weight (in kgs.) based
on their height. We would like an algorithm that gives equally good
predictions whether height is measured in centimeters or kilometers. For
a concrete example, consider the 2D optimization problems: \[
\mathrm{argmin}_{x\in \mathbb{R}^2}\ x^T \Sigma_i x, \quad i = 1,2
\] where \[
\Sigma_1 = \begin{bmatrix}
1 & 0\\
0 & 1
\end{bmatrix}, \quad 
\Sigma_2 = \begin{bmatrix}
1 & 0\\
0 & 100
\end{bmatrix}.
\] Suppose our starting point is the same \(x_0 = (1, 1)^T\) for both
optimization problems. For a fixed learning rate, will gradient descent
perform better on problem \(i=1\) or \(i=2\)? Explain why.

    \hypertarget{feature-scallings-affect-on-the-range-of-convergent-alphas-and-the-rate-of-convergence-during-gradient-descent}{%
\paragraph{\texorpdfstring{Feature scalling's affect on the range of
convergent \(\alpha\)'s and the rate of convergence during gradient
descent}{Feature scalling's affect on the range of convergent \textbackslash{}alpha's and the rate of convergence during gradient descent}}\label{feature-scallings-affect-on-the-range-of-convergent-alphas-and-the-rate-of-convergence-during-gradient-descent}}

    Let, \[
f(x) = x^T \Sigma x = \, \sum_{j}^{2} \sigma_{j} \, x_{j}^{2}
\]

be the function to minimize. And where,

    \begin{equation}
x = \begin{bmatrix}
x_1 \\
x_2 
\end{bmatrix},
\quad
\Sigma = \begin{bmatrix}
\sigma_{1,1} & 0\\
0 & \sigma_{2,2}
\end{bmatrix},
\quad
\sigma = \begin{bmatrix}
\sigma_{1} \\
\sigma_{2}
\end{bmatrix}
= diag(\Sigma)
\end{equation}

    The \(n^{th}\) vector, \(^{(n)}x\), of the fixed point iteration for
this particular function can be written directly from the initial guess,
\(^{(0)}x\), as,

    \begin{equation} \tag{2}
^{(n)}x = B^n \; ^{(0)}x
\end{equation}

\[
\\
\text{ where $B \in \mathbb{R}^{2 \times 2}$ is a diagnol matrix with entries, } \\
\\
\beta_{j,j} = (1 \, - 2 \alpha \sigma_j ) \in \mathbb{R}
\]

    The sequence of vectors,

\[
\lim_{n \rightarrow \infty} \; ^{(n)}x,
\]

converges or diverges to/from the minimum of \(f\) (the root in this
case) depending on: the learning rate, \(\alpha \in \mathbb{R}\), the
initial guess, \(^{(0)}x \in \mathbb{R}^2\) and the maximum absolute
value of the entries of \(\sigma \in \mathbb{R}^2\) (or the maximum
absolute value of the diganol entries of \(\Sigma\)).

\begin{equation}
 sign(\sigma_{j})* \Big( 0 < \alpha < \frac{1}{\sigma_{j}} \Big) \, \text{ converges } \forall \, ^{(0)}x \text{, }
\end{equation}

\begin{equation}
  \text{otherwise the sequence diverges } \forall \, ^{(0)}x \neq \vec{0}
\end{equation}

where, \[
\sigma_j = max \Big( |\sigma_k| \Big) \; for \; \sigma_k \in \sigma
\]

\[ (\; derivation \; later \;) \]

    For the situations \(i=1,2\) defined above where \(^{(0)}x = (1,1)^T\)
you get the following covergent ranges,

\[
 \alpha \in (0, \, 1 ) \; \text{for} \, i = 1,
\]

\[
 \alpha \in (0, \, \frac{1}{100} ) \; \text{for} \, i = 2
\]

So we can see that the unnormalized features for the second optimization
porblem, \$ i = 2 \$ leads to a much narrower range of convergent
alphas.

    \hypertarget{derivation-of-convergence-range}{%
\paragraph{Derivation of convergence
range}\label{derivation-of-convergence-range}}

    Above I said you can write the \(n^{th}\) vector of the fixed-point
iteration dirrectly from the inital vector, \(^{(0)}x\), as,

\begin{equation} \tag{2}
^{(n)}x = B^n \; ^{(0)}x
\end{equation}

\[
\\
\text{ where $B \in \mathbb{R}^{2 \times 2}$ is a diagnol matrix with entries, } \\
\\
\beta_{j,j} = (1 \, - 2 \alpha \sigma_j ) \in \mathbb{R}
\]

    This comes from plugging in \(\frac{\partial f}{\partial x_j}\) to the
equation for an update step of gradient descent for the \textbf{scalar},
\(x_j\),

\[
^{(n)}x_j = ^{(n-1)}x_j - \alpha  \frac{\partial f(\,^{(n-1)}x_j)}{\partial x_j},
\]

\[
\frac{\partial }{\partial x_j} f(x) \, = \, 2 \sigma_{j} x_{j}
\]

    \[
\Rightarrow \quad ^{(n)}x_j = ^{(n-1)}x_j - 2 \alpha \sigma ^{(n-1)} x_j \; = \; (1 - 2 \alpha \sigma_j) ^{(n-1)} x_j \; = \; \beta_{j,j} \, ^{(n-1)} x_j
\]

Where the \(j^{th}\) element if the \(n^{th}\) term of the iteration can
be written directly given, \(^{(0)}x_j\), as

\[
^{(n)}x_j = \; {\beta_{j,j}}^n \, ^{(0)} x_j
\]

    And similarly the \(n^{th}\) \textbf{vector}, \(^{(n)}x\), can be
written,

\begin{equation} \tag{2}
^{(n)}x = B^n \; ^{(0)}x
\end{equation}

    Given the initial guess, \(^{(0)}x = (1,1)^{T}\), the equation above
simplifies to,

\begin{equation}
^{(n)}x = 
\begin{bmatrix}
{\beta_{1,1}}^{n} & 0\\
0 & {\beta_{2,2}}^{n}
\end{bmatrix}
\begin{bmatrix}
1 \\
1
\end{bmatrix}
=
\begin{bmatrix}
{\beta_{1,1}}^{n}\\
{\beta_{2,2}}^{n}
\end{bmatrix}
\end{equation}

    Since we know the function,
\(f(x) = \, \sum_{j}^{2} \sigma_{j,j} x_{j}^{2}\), has a minimum at
\(x = \vec{0}\), then we can see that the iteratation is only gaurenteed
to converge for \(||B||_1 < 1\). With initial guess,
\(^{(0)}x = \vec{1}\), the iteration will \textbf{only} converge if
\(||B||_1 < 1\).

    \begin{equation}
\lim_{n \rightarrow \infty} \, ^{(n)}x = \vec{0} \; \; iff \; ||B||_1 < 1, \quad for \; ^{(0)}x \neq \vec{0}
\end{equation}

    So the convergence is solely dependent on the absolute values of the
diagonal element of \(B\) being strictly less than \(1\). Then the
convergent range of \(\alpha\)'s is determined solely by the largest
absolute value diagonal entry of \(\Sigma\).

    \[
|\beta_{j,j}| = |1 - 2 \alpha \sigma_{j,j}| < 1, \; \forall j
\]

\[
\Rightarrow -1 < 1 - 2\alpha \sigma_{j,j} < 1
\]

\[
\Rightarrow 0 < \alpha \sigma_{j,j} < 1
\]

\begin{equation}
 sign(\sigma_{j})* \Big( 0 < \alpha < \frac{1}{\sigma_{j}} \Big) \, \text{ converges } \forall \, ^{(0)}x \text{, }
\end{equation}

\[
\sigma_j = max \Big( |\sigma_k| \Big) \; for \; \sigma_k \in \sigma
\]

    Additionally the large variation in the sizes of \(\sigma\)'s leads to a
large varaition in the rates of convergence for the different elements
of \(x\). This is especially problematic as the slower convergence rate
(larger \(\beta\) or larger \(\mu\)) dictates the time until
convergence.

\begin{equation}
\lim_{n \rightarrow \infty} \frac{|^{(n+1)}x_{j} - L|}{|^{(n)}x_{j} - L|} = \lim_{n \rightarrow \infty} \frac{|^{(n+1)}x_{j}|}{|^{(n)}x_{j}|} = 1 - 2 \alpha \sigma_{j,j} = \beta_{j,j}
\end{equation}

    \[
\mu_j(\alpha) = 1 - 2 \alpha \sigma_{j,j} = \beta_{j,j}
\]

\[
\mu(\alpha) = max(\mu_1(\alpha), \mu_2(\alpha))
\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}4}]:} \PY{n}{sigma1\PYZus{}p1}\PY{p}{,} \PY{n}{sigma2\PYZus{}p1} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{1}
        \PY{n}{sigma1\PYZus{}p2}\PY{p}{,} \PY{n}{sigma2\PYZus{}p2} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{100}
        
        \PY{n}{alphas\PYZus{}range\PYZus{}1} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mf}{0.0001}\PY{p}{,}\PY{l+m+mf}{0.9999}\PY{p}{,}\PY{n}{num}\PY{o}{=}\PY{l+m+mi}{10000}\PY{p}{)}
        \PY{n}{alphas\PYZus{}range\PYZus{}2} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mf}{0.0001}\PY{p}{,}\PY{l+m+mf}{0.0099}\PY{p}{,}\PY{n}{num}\PY{o}{=}\PY{l+m+mi}{10000}\PY{p}{)}
        
        \PY{n}{mu1\PYZus{}p1} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{abs}\PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{o}{*}\PY{n}{sigma1\PYZus{}p1}\PY{o}{*}\PY{n}{alphas\PYZus{}range\PYZus{}1}\PY{p}{)}
        \PY{n}{mu2\PYZus{}p1} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{abs}\PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{o}{*}\PY{n}{sigma2\PYZus{}p1}\PY{o}{*}\PY{n}{alphas\PYZus{}range\PYZus{}1}\PY{p}{)}
        \PY{n}{mus\PYZus{}p1} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{maximum}\PY{p}{(}\PY{n}{mu1\PYZus{}p1}\PY{p}{,}\PY{n}{mu2\PYZus{}p1}\PY{p}{)}
        
        \PY{n}{mu1\PYZus{}p2} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{abs}\PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{o}{*}\PY{n}{sigma1\PYZus{}p2}\PY{o}{*}\PY{n}{alphas\PYZus{}range\PYZus{}2}\PY{p}{)}
        \PY{n}{mu2\PYZus{}p2} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{abs}\PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{o}{*}\PY{n}{sigma2\PYZus{}p2}\PY{o}{*}\PY{n}{alphas\PYZus{}range\PYZus{}2}\PY{p}{)}
        \PY{n}{mus\PYZus{}p2} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{maximum}\PY{p}{(}\PY{n}{mu1\PYZus{}p2}\PY{p}{,}\PY{n}{mu2\PYZus{}p2}\PY{p}{)}
        
        \PY{n}{fig}\PY{p}{,} \PY{n}{axes} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{n}{nrows}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{ncols}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{,} \PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,} \PY{l+m+mi}{7}\PY{p}{)}\PY{p}{)}
        
        \PY{n}{lm1} \PY{o}{=} \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{alphas\PYZus{}range\PYZus{}1}\PY{p}{,} \PY{n}{mus\PYZus{}p1}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{linewidth}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
        \PY{n}{lm2} \PY{o}{=} \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{alphas\PYZus{}range\PYZus{}2}\PY{p}{,} \PY{n}{mus\PYZus{}p2}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{linewidth}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
        
        \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{alphas\PYZus{}range\PYZus{}1}\PY{p}{,} \PY{n}{mus\PYZus{}p1}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b\PYZhy{}\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{linewidth}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
        \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{alphas\PYZus{}range\PYZus{}2}\PY{p}{,} \PY{n}{mus\PYZus{}p2}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{linewidth}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
        
        \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{alphas\PYZus{}range\PYZus{}1}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{mus\PYZus{}p1}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{markersize}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{)}
        \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{alphas\PYZus{}range\PYZus{}1}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{mus\PYZus{}p1}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{w}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{markersize}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
        
        \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{alphas\PYZus{}range\PYZus{}1}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{mus\PYZus{}p1}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{markersize}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{)}
        \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{alphas\PYZus{}range\PYZus{}1}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{mus\PYZus{}p1}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{w}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{markersize}\PY{o}{=}\PY{l+m+mi}{4}\PY{p}{)}
        
        \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{alphas\PYZus{}range\PYZus{}1}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{mus\PYZus{}p1}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{markersize}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{)}
        \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{alphas\PYZus{}range\PYZus{}1}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{mus\PYZus{}p1}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{w}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{markersize}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
        
        \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{alphas\PYZus{}range\PYZus{}1}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{mus\PYZus{}p1}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{markersize}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{)}
        \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{alphas\PYZus{}range\PYZus{}1}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{mus\PYZus{}p1}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{w}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{markersize}\PY{o}{=}\PY{l+m+mi}{4}\PY{p}{)}
        
        \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{alphas\PYZus{}range\PYZus{}2}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{mus\PYZus{}p2}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{markersize}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{)}
        \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{alphas\PYZus{}range\PYZus{}2}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{mus\PYZus{}p2}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{w}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{markersize}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
        
        \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{alphas\PYZus{}range\PYZus{}2}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{mus\PYZus{}p2}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{markersize}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{)}
        \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{alphas\PYZus{}range\PYZus{}2}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{mus\PYZus{}p2}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{w}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{markersize}\PY{o}{=}\PY{l+m+mi}{4}\PY{p}{)}
        
        \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{alphas\PYZus{}range\PYZus{}2}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{mus\PYZus{}p2}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{markersize}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{)}
        \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{alphas\PYZus{}range\PYZus{}2}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{mus\PYZus{}p2}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{w}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{markersize}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
        
        \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{alphas\PYZus{}range\PYZus{}2}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{mus\PYZus{}p2}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{r}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{markersize}\PY{o}{=}\PY{l+m+mi}{5}\PY{p}{)}
        \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{alphas\PYZus{}range\PYZus{}2}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{mus\PYZus{}p2}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{w}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{markersize}\PY{o}{=}\PY{l+m+mi}{4}\PY{p}{)}
        
        \PY{c+c1}{\PYZsh{} bounce }
        \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{b}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{markersize}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{)}
        \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{l+m+mf}{0.5}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{color}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{w}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{marker}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{o}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{markersize}\PY{o}{=}\PY{l+m+mi}{2}\PY{p}{)}
        
        
        \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}xlim}\PY{p}{(}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.05}\PY{p}{,}\PY{l+m+mf}{1.05}\PY{p}{)}\PY{p}{)}
        \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}ylim}\PY{p}{(}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.05}\PY{p}{,}\PY{l+m+mf}{1.05}\PY{p}{)}\PY{p}{)}
        
        \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}xlim}\PY{p}{(}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.005}\PY{p}{,}\PY{l+m+mf}{0.106}\PY{p}{)}\PY{p}{)}
        \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}ylim}\PY{p}{(}\PY{p}{(}\PY{l+m+mf}{0.8995}\PY{p}{,}\PY{l+m+mf}{1.005}\PY{p}{)}\PY{p}{)}
        
        \PY{n}{left}\PY{p}{,}\PY{n}{right} \PY{o}{=} \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{get\PYZus{}xlim}\PY{p}{(}\PY{p}{)}
        \PY{n}{low}\PY{p}{,}\PY{n}{high} \PY{o}{=} \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{get\PYZus{}ylim}\PY{p}{(}\PY{p}{)}
        
        \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{arrow}\PY{p}{(} \PY{n}{left}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{right} \PY{o}{\PYZhy{}}\PY{n}{left}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{length\PYZus{}includes\PYZus{}head} \PY{o}{=} \PY{k+kc}{False}\PY{p}{,} \PY{n}{head\PYZus{}width} \PY{o}{=} \PY{l+m+mf}{0.05} \PY{p}{)}
        \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{arrow}\PY{p}{(} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{low}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{high}\PY{o}{\PYZhy{}}\PY{n}{low}\PY{p}{,} \PY{n}{length\PYZus{}includes\PYZus{}head} \PY{o}{=} \PY{k+kc}{False}\PY{p}{,} \PY{n}{head\PYZus{}width} \PY{o}{=} \PY{l+m+mf}{0.05} \PY{p}{)} 
        
        \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{arrow}\PY{p}{(} \PY{n}{left}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{right} \PY{o}{\PYZhy{}}\PY{n}{left}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{length\PYZus{}includes\PYZus{}head} \PY{o}{=} \PY{k+kc}{False}\PY{p}{,} \PY{n}{head\PYZus{}width} \PY{o}{=} \PY{l+m+mf}{0.05} \PY{p}{)}
        \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{arrow}\PY{p}{(} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{low}\PY{p}{,} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{high}\PY{o}{\PYZhy{}}\PY{n}{low}\PY{p}{,} \PY{n}{length\PYZus{}includes\PYZus{}head} \PY{o}{=} \PY{k+kc}{False}\PY{p}{,} \PY{n}{head\PYZus{}width} \PY{o}{=} \PY{l+m+mf}{0.05} \PY{p}{)} 
        
        
        \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{grid}\PY{p}{(}\PY{p}{)}
        \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{grid}\PY{p}{(}\PY{p}{)}
        
        \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZdl{}}\PY{l+s+s1}{\PYZbs{}}\PY{l+s+s1}{mu(}\PY{l+s+se}{\PYZbs{}\PYZbs{}}\PY{l+s+s1}{alpha) }\PY{l+s+s1}{\PYZbs{}}\PY{l+s+s1}{; wide }\PY{l+s+s1}{\PYZbs{}}\PY{l+s+s1}{, view\PYZdl{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZdl{}}\PY{l+s+s1}{\PYZbs{}}\PY{l+s+s1}{mu(}\PY{l+s+se}{\PYZbs{}\PYZbs{}}\PY{l+s+s1}{alpha) }\PY{l+s+s1}{\PYZbs{}}\PY{l+s+s1}{; zoomed }\PY{l+s+s1}{\PYZbs{}}\PY{l+s+s1}{, view\PYZdl{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
        
        \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdl{}}\PY{l+s+se}{\PYZbs{}\PYZbs{}}\PY{l+s+s2}{alpha\PYZdl{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdl{}}\PY{l+s+se}{\PYZbs{}\PYZbs{}}\PY{l+s+s2}{alpha\PYZdl{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        
        \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdl{}}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{mu\PYZdl{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n}{axes}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdl{}}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{mu\PYZdl{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        
        \PY{n}{lines} \PY{o}{=} \PY{p}{(}\PY{n}{lm1}\PY{p}{,} \PY{n}{lm2}\PY{p}{)}
        \PY{n}{labels} \PY{o}{=} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdl{}}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{Sigma\PYZus{}1 }\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{, rate }\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{, of }\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{, convergence\PYZcb{}\PYZdl{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdl{}}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{Sigma\PYZus{}2 }\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{, rate }\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{, of }\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{, convergence\PYZdl{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        
        \PY{n}{fig}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{lines}\PY{p}{,}  
                   \PY{n}{labels}\PY{o}{=}\PY{n}{labels}\PY{p}{,}  
                   \PY{n}{loc}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{center right}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
                   \PY{n}{borderaxespad}\PY{o}{=}\PY{l+m+mf}{0.5}\PY{p}{,}
                   \PY{n}{title}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdl{}}\PY{l+s+s2}{\PYZbs{}}\PY{l+s+s2}{mu(}\PY{l+s+se}{\PYZbs{}\PYZbs{}}\PY{l+s+s2}{alpha)\PYZdl{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
                   \PY{n}{fontsize}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{x\PYZhy{}large}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}
                   \PY{n}{framealpha}\PY{o}{=}\PY{l+m+mf}{1.0}\PY{p}{,}
                   \PY{n}{shadow}\PY{o}{=}\PY{l+m+mf}{0.5}
                   \PY{p}{)}
        
        \PY{n}{fig}\PY{o}{.}\PY{n}{tight\PYZus{}layout}\PY{p}{(}\PY{p}{)}
        
        \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
/anaconda3/lib/python3.5/site-packages/matplotlib/legend.py:1363: UserWarning: You have mixed positional and keyword arguments, some input may be discarded.
  warnings.warn("You have mixed positional and keyword "

    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_36_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    From the above graphs you can see that not only is the range of
\(\alpha\)'s that will lead to convergence restricted when there is a
high varience between feature scales, but also (in the given example)
the number of steps needed for the algorithm to converge is confined to
the worst region of the graph (when \(\mu \approx 1\)).

    \hypertarget{problem-2-logistic-regression-60-pts-total}{%
\subsection{Problem 2: Logistic Regression (60 pts
total)}\label{problem-2-logistic-regression-60-pts-total}}

In this problem you will implement all of the steps that are taken care
of whenever the ``fit'' function from sci-kit is called.

    \hypertarget{part-a-15-pts-logisitc-unit}{%
\subsubsection{Part (a) (15 pts) Logisitc
Unit}\label{part-a-15-pts-logisitc-unit}}

For binary classification \(y \in \{0,1\}\) we will model the posterior
probability \(p(y = 1 | x)\) with the logistic unit \[
h(x; w) = \frac{1}{1 + \exp(-w^Tx)}
\] We will use the convention that \(x_0 = 1\). Implement this function
below using the skeletal outline as a guide. Suppose we use a
discriminant function \(f(x)\) which assigns the label \(y = 1\) if
\(p(y = 1 | x) \ge 0.5\) and \(0\) otherwise. In other words, with a
discriminant function we do not need the posterior probabilities but
rather skip straight to the classification.

Give a geometric interpretation of our classifier once the parameters
\(w\) have been learned. What does the angle between \(w\) and \(x\)
tell us about the predicted class value? For which angles do we predict
\(y=1\)?

    \hypertarget{geometrix-interpretation-of-classifier}{%
\paragraph{Geometrix interpretation of
classifier}\label{geometrix-interpretation-of-classifier}}

    \[
   class = 
\begin{cases}
    1       &   \text{if } w^Tx \geq 0, \\
    0       &   \text{if }  w^Tx < 0    \\
\end{cases}
\]

    Let, \(w,x \in \mathbb{R}^n\).

Geometrically, \(w\) represents a normal vector to a class decision
boundy, an \(n-1\) dimensional hyper-plane that passes through the
origin, and so is solely determined by its normal vector, \(w\). Points
that fall on the side of the hyper-plane that \(w\) is pointing are in
class one (or on it), while the points falling on the other side of the
plane are in class zero.

\[
w^T \cdot ( v - \vec{0} ) = 0
\]

\[
\Rightarrow \; w^T v = 0
\]

Any vector, \(v\), that satisfies the above equation lays on the
hyper-plane defined by \(w\) that acts as a decision boundry for the two
different classes.

However, by setting \(x_0 = 1\) we restrict the vector that points to
different data points, \(x\), to lay on a second \(n-1\) dimensional
hyper-plane. This second hyper-planes normal vector, \(e_0\), (in which
all data points lay) has entries,

\[
e_0 = 1, 
\] \[
e_i = 0 \; \text{  for} \,i = 1, 2, \ldots, n-1
\]

\(e_0\) is also a point on this second plane so its equation is,

\[
 e_0^Tv = 1
\]

    Then the intersection between these two hyper-planes becomes the actual
decision boundry for the data points and has \(n-2\) dimensions.

    Then define, \(D \in \mathbb{R}^{2 \times n}\) and
\(y \in \mathbb{R}^2\), to determine the points that lay on the
intersection descision boundry, by whether or not they satisfy the
equation,

    \[ y = Dv \]

    \[
D = \begin{bmatrix}
e_0^T \\
w^T
\end{bmatrix},
\quad
y = \begin{bmatrix}
1 \\
0
\end{bmatrix}
\]

However since we set all the \(x_0\) values equal to one the top half of
this system is redundant. What makes this usefull though is that by
changing all \(n\) entries of the \(w\) vector, \textbf{any} \(n-2\)
dimensional hyper-plane located within the \(n-1\) dimensional
hyper-plane in which all \(x\) vectors are confined can become a
solution to the above system. Meaning that the decsion boundry that
seperates the classes does not have to pass through the origin.
Visually,

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}5}]:} \PY{k+kn}{from} \PY{n+nn}{IPython}\PY{n+nn}{.}\PY{n+nn}{display} \PY{k}{import} \PY{n}{Image}
        \PY{n}{Image}\PY{p}{(}\PY{n}{filename}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{bias\PYZus{}term.jpeg}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} 
\end{Verbatim}

\texttt{\color{outcolor}Out[{\color{outcolor}5}]:}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_47_0.jpeg}
    \end{center}
    { \hspace*{\fill} \\}
    

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}6}]:} \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{Input:}
        \PY{l+s+sd}{    w: weight vector}
        \PY{l+s+sd}{    X: data}
        
        \PY{l+s+sd}{Return:}
        \PY{l+s+sd}{    logits : h(x;w)}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        
        \PY{c+c1}{\PYZsh{} avoids division by zero from overflow}
        \PY{n}{epsilon} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{finfo}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{float32}\PY{p}{)}\PY{o}{.}\PY{n}{eps}
        
        \PY{k}{def} \PY{n+nf}{logistic\PYZus{}unit}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{w}\PY{p}{)}\PY{p}{:}
            \PY{n}{a} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{exp}\PY{p}{(}\PY{o}{\PYZhy{}}\PY{n}{np}\PY{o}{.}\PY{n}{dot}\PY{p}{(}\PY{n}{X}\PY{p}{,}\PY{n}{w}\PY{p}{)}\PY{p}{)} \PY{o}{+} \PY{l+m+mi}{2}\PY{o}{*}\PY{n}{epsilon}
            \PY{n}{d} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{n}{a}\PY{o}{.}\PY{n}{shape}\PY{p}{)} \PY{o}{+} \PY{n}{a}
            \PY{n}{logits} \PY{o}{=} \PY{l+m+mf}{1.0}\PY{o}{/}\PY{n}{d}
            \PY{k}{return} \PY{n}{logits}
\end{Verbatim}


    \hypertarget{part-b-10-pts-deriving-the-loss-function}{%
\subsubsection{Part (b) (10 pts) Deriving the loss
function}\label{part-b-10-pts-deriving-the-loss-function}}

We have implicitly made the assumption that
\(y|x \sim \mathrm{Bernoulli}(h(x;w))\). If we have an iid dataset
\(\{(x_i,y_i)\}_{i=1}^N\), then we can write the data likelihood as \[
p(\vec{y}| X, w) = \prod_{i=1}^N p(y_i | x_i, w)
\] We can learn the parameter \(w\) by maximizing this probability
(i.e.~we find the MLE) or equivalently minimizing
\(J(w) = -\log p(\vec{y} | X, w)\). Derive the loss function \(J(w)\)
step-by-step and implement it using the skeletal outline below.

    \hypertarget{loss-function-derivation}{%
\paragraph{Loss function derivation}\label{loss-function-derivation}}

Given the assumtion that \(y|x \sim \mathrm{Bernoulli}(h(x;w))\), we can
write the probability of a single data point using the equation for the
probability mass function of a Bernouli distribution,

\[
 p(y_i | x_i, w) = h(x_i;w)^{y_i} (1 - h(x_i;w))^{1-y_i},
\]

\[
 y_i \in \{ 0, 1 \}
\]

Substituting this probability mass function into the data likelihood
equation,

\[
p(\vec{y} | X, w) = \prod_{i=1}^N h(x_i;w)^{y_i} (1 - h(x_i;w))^{1-y_i}
\]

The log liklehood (with the same MLE as log function is monotonically
increasing) is then,

\[
log \, p(\vec{y} | X, w) = \sum_{i=1}^N log \, p(y_i | x_i, w) =
\]

\[
 = \sum_{i=1}^N \, y_i log \, h(x_i;w) + (1-y_i) log \, (1 - h(x_i;w))
\]

Taking the negitive of this function we get the cost funciton, \(J\),

\[
J(w) = -\log p(\vec{y} | X, w) = \sum_{i=1}^N \, -y_i log \, h(x_i;w) - (1-y_i) log \, (1 - h(x_i;w))
\]

And,

\[
MLE_{\hat{w}} = argmin_w J(w)
\]

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}7}]:} \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        \PY{l+s+sd}{Input:}
        \PY{l+s+sd}{    w : weight vector}
        \PY{l+s+sd}{    X : dataset features}
        \PY{l+s+sd}{    y : dataset targets}
        
        \PY{l+s+sd}{Return:}
        \PY{l+s+sd}{    J : loss of w given X,y}
        \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{}}
        
        \PY{k}{def} \PY{n+nf}{loss}\PY{p}{(}\PY{n}{w}\PY{p}{,} \PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{)}\PY{p}{:}
            \PY{n}{h} \PY{o}{=} \PY{n}{logistic\PYZus{}unit}\PY{p}{(}\PY{n}{X}\PY{p}{,}\PY{n}{w}\PY{p}{)}
            \PY{n}{J} \PY{o}{=} \PY{p}{(} \PY{o}{\PYZhy{}}\PY{n}{y} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{n}{h}\PY{p}{)} \PY{o}{\PYZhy{}} \PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{y}\PY{p}{)} \PY{o}{*} \PY{n}{np}\PY{o}{.}\PY{n}{log}\PY{p}{(}\PY{l+m+mi}{1} \PY{o}{\PYZhy{}} \PY{n}{h}\PY{p}{)} \PY{p}{)}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{p}{)}
            \PY{c+c1}{\PYZsh{}J = ( \PYZhy{}y * np.log(h) \PYZhy{} (1 \PYZhy{} y) * np.log(1 \PYZhy{} h) ).sum()}
            \PY{k}{return} \PY{n}{J}
\end{Verbatim}


    \hypertarget{part-c-5-pts-splitting-the-data-for-training-and-testing}{%
\subsubsection{Part (c) (5 pts) Splitting the data for training and
testing}\label{part-c-5-pts-splitting-the-data-for-training-and-testing}}

Now we'll actually learn a logistic regression model for some synthetic
data.

First split the dataset into a training, validation, and test set. Use a
40/40/20 split (roughly 40/40/20 is fine).

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}8}]:} \PY{c+c1}{\PYZsh{}}
        \PY{c+c1}{\PYZsh{} generate data}
        \PY{c+c1}{\PYZsh{}}
        
        \PY{k}{def} \PY{n+nf}{generate\PYZus{}data}\PY{p}{(}\PY{n}{n\PYZus{}samples} \PY{o}{=} \PY{l+m+mi}{10000}\PY{p}{,} \PY{n}{centers} \PY{o}{=} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{n\PYZus{}features} \PY{o}{=} \PY{l+m+mi}{20}\PY{p}{,} \PY{n}{cluster\PYZus{}std} \PY{o}{=} \PY{l+m+mi}{20}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{} Generate fake data and return X,y \PYZdq{}\PYZdq{}\PYZdq{}}
        
            \PY{c+c1}{\PYZsh{} We first generate some fake data.}
            \PY{n}{X}\PY{p}{,} \PY{n}{y} \PY{o}{=} \PY{n}{make\PYZus{}blobs}\PY{p}{(}\PY{n}{n\PYZus{}samples} \PY{o}{=} \PY{n}{n\PYZus{}samples}\PY{p}{,} \PY{n}{centers} \PY{o}{=} \PY{l+m+mi}{2}\PY{p}{,} \PY{n}{n\PYZus{}features} \PY{o}{=} \PY{l+m+mi}{20}\PY{p}{,} \PY{n}{cluster\PYZus{}std} \PY{o}{=} \PY{l+m+mi}{20}\PY{p}{)}
        
            \PY{c+c1}{\PYZsh{} We\PYZsq{}ll also augment the data so that x\PYZus{}0 = 1 for the intercept term.}
            \PY{n}{X} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{X}\PY{p}{)}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}\PY{p}{,} \PY{n}{X}\PY{p}{,} \PY{n}{axis} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{)}
            
            \PY{k}{return} \PY{n}{X}\PY{p}{,}\PY{n}{y}
        
        \PY{c+c1}{\PYZsh{}}
        \PY{c+c1}{\PYZsh{} split the data}
        \PY{c+c1}{\PYZsh{}}
        
        \PY{k}{def} \PY{n+nf}{split\PYZus{}data}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{y}\PY{p}{,} \PY{n}{p\PYZus{}train}\PY{o}{=}\PY{l+m+mf}{0.8}\PY{p}{,} \PY{n}{p\PYZus{}val}\PY{o}{=}\PY{l+m+mf}{0.0}\PY{p}{,} \PY{n}{p\PYZus{}test}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{} Split data and return data dictionary \PYZdq{}\PYZdq{}\PYZdq{}}
            
            \PY{c+c1}{\PYZsh{}}
            \PY{c+c1}{\PYZsh{} set split percentages}
            \PY{c+c1}{\PYZsh{}}
            
            \PY{k}{assert} \PY{n}{p\PYZus{}train} \PY{o}{+} \PY{n}{p\PYZus{}val} \PY{o}{+} \PY{n}{p\PYZus{}test} \PY{o}{==} \PY{l+m+mi}{1}
        
            \PY{n}{data} \PY{o}{=} \PY{p}{\PYZob{}}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val}\PY{l+s+s1}{\PYZsq{}} \PY{p}{:} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}\PY{p}{,}
                \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
            \PY{p}{\PYZcb{}}
        
            \PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{p}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{p}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{p}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{p\PYZus{}train}\PY{p}{,} \PY{n}{p\PYZus{}val}\PY{p}{,} \PY{n}{p\PYZus{}test}
        
            \PY{c+c1}{\PYZsh{}}
            \PY{c+c1}{\PYZsh{} split the data}
            \PY{c+c1}{\PYZsh{}}
            
            \PY{n}{keys} \PY{o}{=} \PY{p}{[}\PY{n}{k} \PY{k}{for} \PY{n}{k}\PY{p}{,}\PY{n}{v} \PY{o+ow}{in} \PY{n}{data}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{]}
            \PY{n}{p} \PY{o}{=} \PY{p}{[}\PY{n}{data}\PY{p}{[}\PY{n}{k}\PY{p}{]}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{p}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{k}{for} \PY{n}{k}\PY{p}{,}\PY{n}{v} \PY{o+ow}{in} \PY{n}{data}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{]}
            \PY{n}{split\PYZus{}indices} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{choice}\PY{p}{(}\PY{n}{keys}\PY{p}{,} \PY{n}{size}\PY{o}{=}\PY{n}{y}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{,} \PY{n}{p}\PY{o}{=}\PY{n}{p}\PY{p}{)}
        
            \PY{k}{for} \PY{n}{k}\PY{p}{,}\PY{n}{v} \PY{o+ow}{in} \PY{n}{data}\PY{o}{.}\PY{n}{items}\PY{p}{(}\PY{p}{)}\PY{p}{:}
                \PY{n}{k\PYZus{}indices} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{where}\PY{p}{(}\PY{n}{split\PYZus{}indices} \PY{o}{==} \PY{n}{k}\PY{p}{)}
                \PY{n}{data}\PY{p}{[}\PY{n}{k}\PY{p}{]}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{data}\PY{p}{[}\PY{n}{k}\PY{p}{]}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{X}\PY{p}{[}\PY{n}{k\PYZus{}indices}\PY{p}{]}\PY{p}{,} \PY{n}{y}\PY{p}{[}\PY{n}{k\PYZus{}indices}\PY{p}{]}
                
            \PY{c+c1}{\PYZsh{} return data dictionary}
            \PY{k}{return} \PY{n}{data}
\end{Verbatim}


    \hypertarget{part-d-20pts}{%
\subsubsection{Part (d) (20pts)}\label{part-d-20pts}}

Now use your gradient descent function to learn the parameters \(w\)
using 1000 iterations. You may choose the learning rate and initial
parameters \(w_0\) for this problem. Compare both the fixed learning
rate and variable learning rate gradient descents side by side (i.e.~2
subplots).

For each method, plot the loss of both the training set and the
validation set. Make sure your plots are labeled properly with \(x\) and
\(y\) axis labels, a legend, title, and different colors to distinguish
the two curves on each subplot.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}9}]:} \PY{k}{def} \PY{n+nf}{fit}\PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{loss\PYZus{}function}\PY{o}{=}\PY{n}{loss}\PY{p}{,} \PY{n}{iterations}\PY{o}{=}\PY{l+m+mi}{1000}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{0.01}\PY{p}{,} \PY{n}{optimizer}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gd}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{seed}\PY{o}{=}\PY{l+m+mi}{0}\PY{p}{,} \PY{n}{histories}\PY{o}{=}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)}\PY{p}{:}
            \PY{l+s+sd}{\PYZdq{}\PYZdq{}\PYZdq{} \PYZdq{}\PYZdq{}\PYZdq{}}
            \PY{k}{assert} \PY{n}{optimizer} \PY{o+ow}{in} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gd}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gd\PYZus{}variable}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
            
            \PY{c+c1}{\PYZsh{} Generate a random initial weight vector}
            \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{seed}\PY{p}{(}\PY{n}{seed}\PY{p}{)}
            \PY{n}{w0} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{random}\PY{o}{.}\PY{n}{rand}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)} \PY{o}{+} \PY{l+m+mf}{0.5}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{ones}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}
            
            \PY{c+c1}{\PYZsh{} init history dict and arrays}
            \PY{n}{history} \PY{o}{=} \PY{p}{\PYZob{}}\PY{p}{\PYZcb{}}
            
            \PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{optimizer}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{optimizer}
            \PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{iterations}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{iterations}
            \PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{lr}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{lr}
            \PY{n}{history}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{seed}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{o}{=} \PY{n}{seed}
            
            \PY{n}{history\PYZus{}p} \PY{o}{=} \PY{l+m+mf}{0.1}
            \PY{n}{history\PYZus{}spacing} \PY{o}{=} \PY{n+nb}{int}\PY{p}{(}\PY{n}{iterations}\PY{o}{*}\PY{n}{history\PYZus{}p}\PY{p}{)}
            \PY{n}{history\PYZus{}len} \PY{o}{=} \PY{n+nb}{int}\PY{p}{(}\PY{n}{iterations}\PY{o}{*}\PY{n}{history\PYZus{}p}\PY{p}{)}
            
            \PY{k}{for} \PY{n}{k} \PY{o+ow}{in} \PY{n}{histories}\PY{p}{:}
                \PY{n}{history}\PY{p}{[}\PY{n}{k}\PY{p}{]} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{n}{shape}\PY{o}{=}\PY{p}{(}\PY{n}{history\PYZus{}len}\PY{p}{,}\PY{p}{)}\PY{p}{)}
                
            \PY{c+c1}{\PYZsh{} wrap loss function for gradient descent training}
            \PY{n}{loss\PYZus{}gd} \PY{o}{=} \PY{k}{lambda} \PY{n}{wj}\PY{p}{:} \PY{n}{loss\PYZus{}function}\PY{p}{(} \PY{n}{wj}\PY{p}{,} \PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]} \PY{p}{)}
                
            \PY{c+c1}{\PYZsh{} train}
            \PY{n}{sub\PYZus{}iterations} \PY{o}{=} \PY{n}{history\PYZus{}spacing}
            \PY{n}{wi} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{n}{w0}\PY{p}{)}
            \PY{k}{for} \PY{n}{hist\PYZus{}idx} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{history\PYZus{}len}\PY{p}{)}\PY{p}{:}
                
                \PY{c+c1}{\PYZsh{} duplicate weights}
                \PY{n}{wi} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{n}{wi}\PY{p}{)}
                
                \PY{c+c1}{\PYZsh{}}
                \PY{c+c1}{\PYZsh{} perform sub iterations for chosen optomizer}
                \PY{c+c1}{\PYZsh{}}
                
                \PY{k}{if} \PY{n}{optimizer} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gd\PYZus{}variable}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
                    
                    \PY{c+c1}{\PYZsh{} compute offset}
                    \PY{n}{iteration\PYZus{}offset} \PY{o}{=} \PY{n}{hist\PYZus{}idx}\PY{o}{*}\PY{n}{history\PYZus{}spacing}
                    
                    \PY{c+c1}{\PYZsh{} run variable gradient descent}
                    \PY{n}{wi} \PY{o}{=} \PY{n}{gradient\PYZus{}descent\PYZus{}variable}\PY{p}{(}\PY{n}{loss\PYZus{}gd}\PY{p}{,} \PY{n}{wi}\PY{p}{,} \PY{n}{lr}\PY{p}{,} \PY{n}{sub\PYZus{}iterations}\PY{p}{,} \PY{n}{iteration\PYZus{}offset}\PY{o}{=}\PY{n}{iteration\PYZus{}offset}\PY{p}{)}
                
                \PY{k}{else}\PY{p}{:}
                    
                    \PY{c+c1}{\PYZsh{} run gradient descent}
                    \PY{n}{wi} \PY{o}{=} \PY{n}{gradient\PYZus{}descent\PYZus{}fixed}\PY{p}{(}\PY{n}{loss\PYZus{}gd}\PY{p}{,} \PY{n}{wi}\PY{p}{,} \PY{n}{lr}\PY{p}{,} \PY{n}{sub\PYZus{}iterations}\PY{p}{)}
                
                
                \PY{c+c1}{\PYZsh{}}
                \PY{c+c1}{\PYZsh{} save loss evaluations to history dict}
                \PY{c+c1}{\PYZsh{}}
                
                \PY{k}{for} \PY{n}{k} \PY{o+ow}{in} \PY{n}{histories}\PY{p}{:}
                    \PY{n}{history}\PY{p}{[}\PY{n}{k}\PY{p}{]}\PY{p}{[}\PY{n}{hist\PYZus{}idx}\PY{p}{]} \PY{o}{=} \PY{n}{loss}\PY{p}{(} \PY{n}{wi}\PY{p}{,} \PY{n}{data}\PY{p}{[}\PY{n}{k}\PY{p}{]}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{data}\PY{p}{[}\PY{n}{k}\PY{p}{]}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{)} 
                
            \PY{c+c1}{\PYZsh{} return final weights and history dict }
            \PY{k}{return} \PY{p}{[}\PY{n}{wi}\PY{p}{,} \PY{n}{history}\PY{p}{]}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}10}]:} \PY{k}{def} \PY{n+nf}{plot\PYZus{}histories}\PY{p}{(}\PY{n}{histories\PYZus{}list}\PY{p}{)}\PY{p}{:}
             
             \PY{n}{n\PYZus{}row} \PY{o}{=} \PY{l+m+mi}{1}
             \PY{n}{n\PYZus{}col} \PY{o}{=} \PY{n+nb}{len}\PY{p}{(}\PY{n}{histories\PYZus{}list}\PY{p}{)}
         
             \PY{k}{for} \PY{n}{c} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,}\PY{n}{n\PYZus{}col}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
                 \PY{n}{hist} \PY{o}{=} \PY{n}{histories\PYZus{}list}\PY{p}{[}\PY{n}{c}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
                 
                 \PY{n}{optimizer} \PY{o}{=} \PY{n}{hist}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{optimizer}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
                 
                 \PY{n}{train\PYZus{}loss} \PY{o}{=} \PY{n}{hist}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
                 \PY{n}{val\PYZus{}loss} \PY{o}{=} \PY{n}{hist}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
                 
                 \PY{n}{plt}\PY{o}{.}\PY{n}{subplot}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{n}{n\PYZus{}col}\PY{p}{,} \PY{n}{c}\PY{p}{)}
                 \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{train\PYZus{}loss}\PY{p}{)}
                 \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{val\PYZus{}loss}\PY{p}{)}
                 
                 \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZpc{}s}\PY{l+s+s1}{ Loss History}\PY{l+s+s1}{\PYZsq{}} \PY{o}{\PYZpc{}} \PY{n}{optimizer}\PY{p}{)}
                 \PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Loss}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                 \PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{epoch}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
                 
                 \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{train}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{val}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{loc}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{upper right}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
             
             \PY{n}{plt}\PY{o}{.}\PY{n}{tight\PYZus{}layout}\PY{p}{(}\PY{p}{)}
             \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}11}]:} \PY{c+c1}{\PYZsh{}}
         \PY{c+c1}{\PYZsh{} Generate and split data}
         \PY{c+c1}{\PYZsh{}}
         
         \PY{n}{X}\PY{p}{,}\PY{n}{y} \PY{o}{=} \PY{n}{generate\PYZus{}data}\PY{p}{(}\PY{p}{)}
         \PY{n}{data} \PY{o}{=} \PY{n}{split\PYZus{}data}\PY{p}{(}\PY{n}{X}\PY{p}{,}\PY{n}{y}\PY{p}{,}\PY{n}{p\PYZus{}train}\PY{o}{=}\PY{l+m+mf}{0.4}\PY{p}{,}\PY{n}{p\PYZus{}val}\PY{o}{=}\PY{l+m+mf}{0.4}\PY{p}{,}\PY{n}{p\PYZus{}test}\PY{o}{=}\PY{l+m+mf}{0.2}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}12}]:} \PY{c+c1}{\PYZsh{}}
         \PY{c+c1}{\PYZsh{} train fixed learning rate \PYZsq{}gd\PYZsq{} model}
         \PY{c+c1}{\PYZsh{}}
         
         \PY{n}{w\PYZus{}gd}\PY{p}{,} \PY{n}{history\PYZus{}gd} \PY{o}{=} \PY{n}{fit}\PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{0.01}\PY{p}{,} \PY{n}{optimizer}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gd}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{seed}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}13}]:} \PY{c+c1}{\PYZsh{}}
         \PY{c+c1}{\PYZsh{} train variable learning rate \PYZsq{}gd\PYZus{}variable\PYZsq{} model}
         \PY{c+c1}{\PYZsh{}}
         
         \PY{n}{w\PYZus{}gdv}\PY{p}{,} \PY{n}{history\PYZus{}gdv} \PY{o}{=} \PY{n}{fit}\PY{p}{(}\PY{n}{data}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{l+m+mf}{0.4}\PY{p}{,} \PY{n}{optimizer}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gd\PYZus{}variable}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{seed}\PY{o}{=}\PY{l+m+mi}{1}\PY{p}{)}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}14}]:} \PY{c+c1}{\PYZsh{}}
         \PY{c+c1}{\PYZsh{} Plot the validation and training loss for each model}
         \PY{c+c1}{\PYZsh{}}
         
         \PY{n}{histories} \PY{o}{=} \PY{p}{[}\PY{n}{history\PYZus{}gd}\PY{p}{,} \PY{n}{history\PYZus{}gdv}\PY{p}{]}
         \PY{n}{plot\PYZus{}histories}\PY{p}{(}\PY{n}{histories}\PY{p}{)}
\end{Verbatim}


    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{output_60_0.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{part-e-10-pts-evaluating-the-model-on-the-test-set}{%
\subsubsection{Part (e) (10 pts) Evaluating the model on the test
set}\label{part-e-10-pts-evaluating-the-model-on-the-test-set}}

Finally, we have trained our models and are ready to evaluate them on
the test set. For binary classification one way to check our classifier
is to make a confusion matrix of our predictions. \[
C = \begin{bmatrix}
\text{Predict 0, Actual 0} & \text{Predict 0, Actual 1}\\
\text{Predict 1, Actual 0} & \text{Predict 1, Actual 1}
\end{bmatrix}
\] The diagonal elements are the number of samples that are correctly
classified.

Use your trained models to classify samples in the test set according to
whether \(h(x_i; w) \ge 0.5\) or not and print the confusion matrices.
Also print the accuracy rate which is just the percentage of correctly
classified examples for both models.

    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}15}]:} \PY{k}{def} \PY{n+nf}{h}\PY{p}{(} \PY{n}{X}\PY{p}{,} \PY{n}{w} \PY{p}{)}\PY{p}{:}
             \PY{n}{logits} \PY{o}{=} \PY{n}{logistic\PYZus{}unit}\PY{p}{(}\PY{n}{X}\PY{p}{,} \PY{n}{w}\PY{p}{)}
             \PY{n}{b} \PY{o}{=} \PY{n}{logits} \PY{o}{\PYZgt{}}\PY{o}{=} \PY{l+m+mf}{0.5}
             \PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{b}\PY{o}{.}\PY{n}{astype}\PY{p}{(}\PY{n+nb}{int}\PY{p}{)}
             \PY{k}{return} \PY{n}{y\PYZus{}pred}
         
         \PY{k}{def} \PY{n+nf}{confusion\PYZus{}matrix}\PY{p}{(} \PY{n}{y}\PY{p}{,} \PY{n}{y\PYZus{}pred} \PY{p}{)}\PY{p}{:}
             \PY{n}{p\PYZus{}indices} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{where}\PY{p}{(}\PY{n}{y} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{)}
             \PY{n}{n\PYZus{}indices} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{where}\PY{p}{(}\PY{n}{y} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} create class index map}
             \PY{n}{unique\PYZus{}values} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{unique}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{y}\PY{p}{,} \PY{n}{y\PYZus{}pred}\PY{p}{)}\PY{p}{)}
             \PY{n}{num\PYZus{}classes} \PY{o}{=} \PY{n}{unique\PYZus{}values}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
             \PY{n}{cim} \PY{o}{=} \PY{p}{\PYZob{}}\PY{n}{raw}\PY{p}{:}\PY{n}{map\PYZus{}index} \PY{k}{for} \PY{n}{raw}\PY{p}{,} \PY{n}{map\PYZus{}index} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n}{unique\PYZus{}values}\PY{p}{,}\PY{n+nb}{range}\PY{p}{(}\PY{n}{num\PYZus{}classes}\PY{p}{)}\PY{p}{)}\PY{p}{\PYZcb{}}
             
             \PY{c+c1}{\PYZsh{} init confusion matrix}
             \PY{n}{C} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{zeros}\PY{p}{(}\PY{p}{(}\PY{n}{num\PYZus{}classes}\PY{p}{,} \PY{n}{num\PYZus{}classes}\PY{p}{)}\PY{p}{)}
             
         
             \PY{c+c1}{\PYZsh{} fill confusion matrix}
             \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{y}\PY{o}{.}\PY{n}{shape}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{)}\PY{p}{:}
                 \PY{n}{r}\PY{p}{,}\PY{n}{c} \PY{o}{=} \PY{n}{cim}\PY{p}{[}\PY{n}{y\PYZus{}pred}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{]}\PY{p}{,} \PY{n}{cim}\PY{p}{[}\PY{n}{y}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{]}
                 \PY{n}{C}\PY{p}{[}\PY{n}{r}\PY{p}{]}\PY{p}{[}\PY{n}{c}\PY{p}{]} \PY{o}{+}\PY{o}{=} \PY{l+m+mi}{1}
         
             \PY{k}{return} \PY{n}{C}
         
         \PY{k}{def} \PY{n+nf}{accuracy}\PY{p}{(}\PY{n}{confusion\PYZus{}matrix}\PY{p}{)}\PY{p}{:}
             \PY{n}{correct} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{diag}\PY{p}{(}\PY{n}{C}\PY{p}{)}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}
             \PY{n}{total} \PY{o}{=} \PY{n}{C}\PY{o}{.}\PY{n}{flatten}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{sum}\PY{p}{(}\PY{p}{)}
             \PY{k}{return} \PY{n}{correct}\PY{o}{/}\PY{n}{total}
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]
{\color{incolor}In [{\color{incolor}16}]:} \PY{c+c1}{\PYZsh{}}
         \PY{c+c1}{\PYZsh{} Evaluate Models on the Test Set}
         \PY{c+c1}{\PYZsh{}}
         
         \PY{n}{model\PYZus{}names} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gd}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{gd\PYZus{}variable}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}
         \PY{n}{model\PYZus{}weights} \PY{o}{=} \PY{p}{[}\PY{n}{w\PYZus{}gd}\PY{p}{,} \PY{n}{w\PYZus{}gdv}\PY{p}{]} 
         
         \PY{k}{for} \PY{n}{name}\PY{p}{,} \PY{n}{w} \PY{o+ow}{in} \PY{n+nb}{zip}\PY{p}{(}\PY{n}{model\PYZus{}names}\PY{p}{,} \PY{n}{model\PYZus{}weights}\PY{p}{)}\PY{p}{:}
             
             \PY{c+c1}{\PYZsh{} compute model predictions}
             \PY{n}{y\PYZus{}pred} \PY{o}{=} \PY{n}{h}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{X}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{w}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} compute model confusion matrix}
             \PY{n}{C} \PY{o}{=} \PY{n}{confusion\PYZus{}matrix}\PY{p}{(}\PY{n}{data}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{test}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}\PY{p}{,} \PY{n}{y\PYZus{}pred} \PY{p}{)}
             
             \PY{c+c1}{\PYZsh{} compute model accuaracy}
             \PY{n}{acc} \PY{o}{=} \PY{n}{accuracy}\PY{p}{(}\PY{n}{C}\PY{p}{)}
             
             \PY{c+c1}{\PYZsh{}}
             \PY{c+c1}{\PYZsh{} display}
             \PY{c+c1}{\PYZsh{}}
             
             \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{Model optimizer: }\PY{l+s+si}{\PYZpc{}s}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s2}{test accuracy: }\PY{l+s+si}{\PYZpc{}s}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+se}{\PYZbs{}t}\PY{l+s+s2}{confusion matrix: }\PY{l+s+se}{\PYZbs{}n}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+si}{\PYZpc{}s}\PY{l+s+se}{\PYZbs{}n}\PY{l+s+s2}{\PYZdq{}} \PY{o}{\PYZpc{}} \PY{p}{(}\PY{n}{name}\PY{p}{,} \PY{n}{acc}\PY{p}{,} \PY{n}{C}\PY{p}{)}\PY{p}{)}
             
\end{Verbatim}


    \begin{Verbatim}[commandchars=\\\{\}]

Model optimizer: gd

	test accuracy: 0.8431179082387765
	confusion matrix: 

[[819. 145.]
 [173. 890.]]


Model optimizer: gd\_variable

	test accuracy: 0.8273310310804144
	confusion matrix: 

[[735.  93.]
 [257. 942.]]


    \end{Verbatim}


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
